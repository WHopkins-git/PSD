{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Advanced Feature Extraction\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Beyond Simple PSD Ratio\n",
    "\n",
    "While the basic tail-to-total PSD ratio works well, we can extract **100+ timing features** from each waveform for improved discrimination:\n",
    "\n",
    "**Feature Categories:**\n",
    "1. **Multiple Charge Ratios**: Different gate pairs (fast, medium, slow)\n",
    "2. **Rise Time Features**: 10-90%, 20-80%, CFD timing\n",
    "3. **Shape Moments**: Mean, variance, skewness, kurtosis\n",
    "4. **Cumulative Charge**: Time to reach 10%, 20%, ..., 90% of total charge\n",
    "5. **Decay Parameters**: Bi-exponential fit (fast/slow components)\n",
    "6. **Frequency Domain**: FFT, power spectral density\n",
    "7. **Time-over-Threshold**: Duration above various thresholds\n",
    "8. **Template Matching**: Correlation with neutron/gamma templates\n",
    "9. **Gatti Filter**: Optimal matched filter\n",
    "10. **Wavelet Features**: Multi-resolution decomposition\n",
    "\n",
    "### Why So Many Features?\n",
    "\n",
    "**Advantages:**\n",
    "- Machine learning can find optimal combinations\n",
    "- Different features work better at different energies\n",
    "- Redundancy provides robustness to noise\n",
    "- Physics-informed features improve interpretability\n",
    "\n",
    "**Physics Motivation:**\n",
    "- Scintillation process is complex (multiple decay components)\n",
    "- Different features capture different physical phenomena\n",
    "- Energy-dependent quenching affects pulse shape\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "1. Extract timing features from waveforms\n",
    "2. Calculate cumulative charge timestamps\n",
    "3. Perform bi-exponential decay fits\n",
    "4. Extract frequency-domain features\n",
    "5. Implement template matching and Gatti filter\n",
    "6. Analyze feature importance\n",
    "7. Handle quality control (saturation, pile-up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal, optimize, stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "import pywt  # pip install PyWavelets\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_waveform(particle_type, energy_kev, \n",
    "                     sampling_rate_mhz=250, num_samples=368):\n",
    "    \"\"\"Generate bi-exponential scintillation pulse\"\"\"\n",
    "    dt = 1000.0 / sampling_rate_mhz\n",
    "    time = np.arange(num_samples) * dt\n",
    "    \n",
    "    tau_fast = 3.2\n",
    "    tau_slow = 32.0\n",
    "    \n",
    "    if particle_type == 'gamma':\n",
    "        fast_fraction = 0.75\n",
    "    else:  # neutron\n",
    "        fast_fraction = 0.55\n",
    "    \n",
    "    amplitude = energy_kev * 3.0\n",
    "    t0 = 200\n",
    "    \n",
    "    pulse = np.zeros_like(time)\n",
    "    active_time = time - t0\n",
    "    valid = active_time >= 0\n",
    "    \n",
    "    pulse[valid] = amplitude * (\n",
    "        fast_fraction * np.exp(-active_time[valid] / tau_fast) +\n",
    "        (1 - fast_fraction) * np.exp(-active_time[valid] / tau_slow)\n",
    "    )\n",
    "    \n",
    "    baseline = 8192\n",
    "    waveform = baseline - pulse\n",
    "    waveform += np.random.normal(0, 10, num_samples)\n",
    "    waveform = np.clip(waveform, 0, 16383)\n",
    "    \n",
    "    return waveform.astype(int)\n",
    "\n",
    "# Generate example waveforms\n",
    "gamma_wf = generate_waveform('gamma', 500)\n",
    "neutron_wf = generate_waveform('neutron', 500)\n",
    "\n",
    "print(f\"✓ Generated example waveforms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extractor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Comprehensive feature extraction for PSD analysis\n",
    "    Extracts 100+ features from each waveform\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sampling_rate_mhz=250, baseline_samples=50):\n",
    "        self.sampling_rate = sampling_rate_mhz\n",
    "        self.dt = 1000.0 / sampling_rate_mhz  # ns per sample\n",
    "        self.baseline_samples = baseline_samples\n",
    "        \n",
    "        # Will be set during template building\n",
    "        self.neutron_template = None\n",
    "        self.gamma_template = None\n",
    "        self.gatti_weights = None\n",
    "    \n",
    "    def extract_all_features(self, waveform):\n",
    "        \"\"\"\n",
    "        Extract complete feature set from waveform\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        features : dict\n",
    "            Dictionary of all extracted features\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # 1. Quality control\n",
    "        features.update(self._qc_features(waveform))\n",
    "        \n",
    "        if features.get('saturated', False):\n",
    "            return features  # Skip bad pulses\n",
    "        \n",
    "        # 2. Baseline characterization\n",
    "        baseline = np.mean(waveform[:self.baseline_samples])\n",
    "        baseline_rms = np.std(waveform[:self.baseline_samples])\n",
    "        features['baseline_mean'] = baseline\n",
    "        features['baseline_rms'] = baseline_rms\n",
    "        \n",
    "        # 3. Baseline-subtract and normalize\n",
    "        pulse = baseline - waveform\n",
    "        pulse[pulse < 0] = 0\n",
    "        amplitude = np.max(pulse)\n",
    "        \n",
    "        if amplitude < 10:\n",
    "            return features  # Too small\n",
    "        \n",
    "        pulse_norm = pulse / amplitude\n",
    "        features['amplitude'] = amplitude\n",
    "        \n",
    "        # 4. Multiple charge ratios (KEY FEATURES)\n",
    "        features.update(self._charge_ratio_features(pulse))\n",
    "        \n",
    "        # 5. Rise time features\n",
    "        features.update(self._rise_time_features(pulse_norm))\n",
    "        \n",
    "        # 6. Cumulative charge timestamps\n",
    "        features.update(self._cumulative_charge_features(pulse))\n",
    "        \n",
    "        # 7. Shape moments\n",
    "        features.update(self._shape_moments(pulse_norm))\n",
    "        \n",
    "        # 8. Time-over-threshold\n",
    "        features.update(self._tot_features(pulse_norm))\n",
    "        \n",
    "        # 9. Decay fit\n",
    "        features.update(self._decay_fit_features(pulse))\n",
    "        \n",
    "        # 10. Frequency domain\n",
    "        features.update(self._frequency_features(pulse_norm))\n",
    "        \n",
    "        # 11. Template matching (if templates available)\n",
    "        if self.neutron_template is not None:\n",
    "            features.update(self._template_features(pulse_norm))\n",
    "        \n",
    "        # 12. Wavelet features\n",
    "        features.update(self._wavelet_features(pulse))\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _qc_features(self, waveform):\n",
    "        \"\"\"Quality control features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Saturation check\n",
    "        saturated = (waveform <= 10) | (waveform >= 16373)\n",
    "        features['saturated'] = saturated.any()\n",
    "        features['n_saturated_samples'] = saturated.sum()\n",
    "        \n",
    "        # Pile-up detection (count peaks)\n",
    "        deriv = np.diff(waveform)\n",
    "        sign_changes = np.diff(np.sign(deriv))\n",
    "        n_peaks = (sign_changes < 0).sum()\n",
    "        features['n_peaks'] = n_peaks\n",
    "        features['pile_up_likely'] = n_peaks > 2\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _charge_ratio_features(self, pulse):\n",
    "        \"\"\"Multiple charge integration ratios\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Define gate pairs (in ns)\n",
    "        gate_pairs = [\n",
    "            (0, 20),    # Very fast\n",
    "            (0, 60),    # Fast\n",
    "            (0, 200),   # Medium (traditional short)\n",
    "            (0, 800),   # Long (traditional long)\n",
    "            (20, 60),   # Early tail\n",
    "            (60, 200),  # Mid tail\n",
    "            (200, 800), # Late tail\n",
    "        ]\n",
    "        \n",
    "        total_integral = pulse.sum()\n",
    "        \n",
    "        for start_ns, end_ns in gate_pairs:\n",
    "            start_idx = int(start_ns / self.dt)\n",
    "            end_idx = min(int(end_ns / self.dt), len(pulse))\n",
    "            \n",
    "            if end_idx > start_idx:\n",
    "                gate_integral = pulse[start_idx:end_idx].sum()\n",
    "                ratio = gate_integral / total_integral if total_integral > 0 else 0\n",
    "                features[f'Q_ratio_{start_ns}_{end_ns}ns'] = ratio\n",
    "        \n",
    "        # Traditional PSD\n",
    "        short_gate = int(200 / self.dt)\n",
    "        long_gate = int(800 / self.dt)\n",
    "        Q_short = pulse[:short_gate].sum()\n",
    "        Q_long = pulse[:long_gate].sum()\n",
    "        features['psd_traditional'] = (Q_long - Q_short) / Q_long if Q_long > 0 else 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _rise_time_features(self, pulse_norm):\n",
    "        \"\"\"Rise time calculations\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        peak_idx = np.argmax(pulse_norm)\n",
    "        \n",
    "        # Find crossing times\n",
    "        thresholds = {'10': 0.10, '20': 0.20, '50': 0.50, '80': 0.80, '90': 0.90}\n",
    "        crossing_times = {}\n",
    "        \n",
    "        for name, thresh in thresholds.items():\n",
    "            idx = np.where(pulse_norm[:peak_idx] >= thresh)[0]\n",
    "            if len(idx) > 0:\n",
    "                crossing_times[name] = idx[0] * self.dt\n",
    "            else:\n",
    "                crossing_times[name] = 0\n",
    "        \n",
    "        # Rise time combinations\n",
    "        features['rise_10_90'] = crossing_times['90'] - crossing_times['10']\n",
    "        features['rise_20_80'] = crossing_times['80'] - crossing_times['20']\n",
    "        features['rise_10_50'] = crossing_times['50'] - crossing_times['10']\n",
    "        features['time_to_peak'] = peak_idx * self.dt\n",
    "        \n",
    "        # Peak position (fraction of total trace)\n",
    "        features['peak_position_frac'] = peak_idx / len(pulse_norm)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _cumulative_charge_features(self, pulse):\n",
    "        \"\"\"Cumulative charge timestamps\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        cumsum = np.cumsum(pulse)\n",
    "        total_charge = cumsum[-1]\n",
    "        \n",
    "        if total_charge > 0:\n",
    "            for pct in range(10, 100, 10):\n",
    "                threshold = pct / 100.0 * total_charge\n",
    "                idx = np.searchsorted(cumsum, threshold)\n",
    "                \n",
    "                if idx < len(cumsum):\n",
    "                    # Linear interpolation\n",
    "                    if idx > 0:\n",
    "                        frac = (threshold - cumsum[idx-1]) / (cumsum[idx] - cumsum[idx-1] + 1e-10)\n",
    "                        time = (idx - 1 + frac) * self.dt\n",
    "                    else:\n",
    "                        time = 0\n",
    "                else:\n",
    "                    time = len(cumsum) * self.dt\n",
    "                \n",
    "                features[f'charge_t{pct}pct'] = time\n",
    "            \n",
    "            # Charge collection speed\n",
    "            t10 = features['charge_t10pct']\n",
    "            t50 = features['charge_t50pct']\n",
    "            t90 = features['charge_t90pct']\n",
    "            \n",
    "            if t90 > t10:\n",
    "                features['charge_speed_10_50'] = 0.4 / (t50 - t10) if t50 > t10 else 0\n",
    "                features['charge_speed_50_90'] = 0.4 / (t90 - t50) if t90 > t50 else 0\n",
    "                features['charge_asymmetry'] = (t50 - t10) / (t90 - t10)\n",
    "            else:\n",
    "                features['charge_speed_10_50'] = 0\n",
    "                features['charge_speed_50_90'] = 0\n",
    "                features['charge_asymmetry'] = 0.5\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _shape_moments(self, pulse_norm):\n",
    "        \"\"\"Statistical shape moments\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Treat pulse as probability distribution\n",
    "        if pulse_norm.sum() > 0:\n",
    "            features['skewness'] = skew(pulse_norm)\n",
    "            features['kurtosis'] = kurtosis(pulse_norm)\n",
    "            \n",
    "            # Weighted mean time\n",
    "            time = np.arange(len(pulse_norm)) * self.dt\n",
    "            features['mean_time'] = np.average(time, weights=pulse_norm)\n",
    "            features['std_time'] = np.sqrt(np.average((time - features['mean_time'])**2, \n",
    "                                                      weights=pulse_norm))\n",
    "        else:\n",
    "            features['skewness'] = 0\n",
    "            features['kurtosis'] = 0\n",
    "            features['mean_time'] = 0\n",
    "            features['std_time'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _tot_features(self, pulse_norm):\n",
    "        \"\"\"Time-over-threshold features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            above = pulse_norm > thresh\n",
    "            if above.any():\n",
    "                tot = above.sum() * self.dt\n",
    "                first_cross = np.where(above)[0][0] * self.dt\n",
    "                last_cross = np.where(above)[0][-1] * self.dt\n",
    "            else:\n",
    "                tot = 0\n",
    "                first_cross = 0\n",
    "                last_cross = 0\n",
    "            \n",
    "            features[f'tot_{int(thresh*100)}pct'] = tot\n",
    "            features[f'tot_start_{int(thresh*100)}pct'] = first_cross\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _decay_fit_features(self, pulse):\n",
    "        \"\"\"Bi-exponential decay fit\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        peak_idx = np.argmax(pulse)\n",
    "        \n",
    "        if peak_idx < len(pulse) - 50:\n",
    "            tail = pulse[peak_idx:peak_idx+150]\n",
    "            x = np.arange(len(tail)) * self.dt\n",
    "            \n",
    "            def biexp(t, A_fast, tau_fast, A_slow, tau_slow):\n",
    "                return A_fast * np.exp(-t/tau_fast) + A_slow * np.exp(-t/tau_slow)\n",
    "            \n",
    "            try:\n",
    "                p0 = [tail[0]*0.7, 10, tail[0]*0.3, 50]\n",
    "                popt, _ = optimize.curve_fit(biexp, x, tail, p0=p0, maxfev=5000,\n",
    "                                            bounds=([0,1,0,10], [np.inf,100,np.inf,500]))\n",
    "                \n",
    "                A_fast, tau_fast, A_slow, tau_slow = popt\n",
    "                \n",
    "                features['decay_tau_fast'] = tau_fast\n",
    "                features['decay_tau_slow'] = tau_slow\n",
    "                features['decay_A_ratio'] = A_slow / (A_fast + A_slow + 1e-10)\n",
    "                features['decay_tau_ratio'] = tau_slow / (tau_fast + 1e-10)\n",
    "                \n",
    "                # Fit quality\n",
    "                y_pred = biexp(x, *popt)\n",
    "                r2 = 1 - np.sum((tail - y_pred)**2) / (np.sum((tail - np.mean(tail))**2) + 1e-10)\n",
    "                features['decay_fit_r2'] = r2\n",
    "                \n",
    "            except:\n",
    "                features['decay_tau_fast'] = 0\n",
    "                features['decay_tau_slow'] = 0\n",
    "                features['decay_A_ratio'] = 0\n",
    "                features['decay_tau_ratio'] = 0\n",
    "                features['decay_fit_r2'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _frequency_features(self, pulse_norm):\n",
    "        \"\"\"Frequency domain features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # FFT\n",
    "        fft = np.fft.rfft(pulse_norm)\n",
    "        power = np.abs(fft)**2\n",
    "        freqs = np.fft.rfftfreq(len(pulse_norm), d=self.dt/1000)  # MHz\n",
    "        \n",
    "        # Spectral centroid\n",
    "        if power.sum() > 0:\n",
    "            features['spectral_centroid'] = np.average(freqs, weights=power)\n",
    "            features['spectral_variance'] = np.average((freqs - features['spectral_centroid'])**2, \n",
    "                                                       weights=power)\n",
    "        else:\n",
    "            features['spectral_centroid'] = 0\n",
    "            features['spectral_variance'] = 0\n",
    "        \n",
    "        # Power in frequency bands\n",
    "        bands = [(0, 10), (10, 50), (50, 100)]  # MHz\n",
    "        for low, high in bands:\n",
    "            mask = (freqs >= low) & (freqs < high)\n",
    "            band_power = power[mask].sum()\n",
    "            features[f'power_{low}_{high}MHz'] = band_power\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _template_features(self, pulse_norm):\n",
    "        \"\"\"Template matching features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        min_len = min(len(pulse_norm), len(self.neutron_template))\n",
    "        pulse_trunc = pulse_norm[:min_len]\n",
    "        template_n = self.neutron_template[:min_len]\n",
    "        template_g = self.gamma_template[:min_len]\n",
    "        \n",
    "        # Correlation\n",
    "        corr_n = np.corrcoef(pulse_trunc, template_n)[0, 1]\n",
    "        corr_g = np.corrcoef(pulse_trunc, template_g)[0, 1]\n",
    "        \n",
    "        features['template_n_corr'] = corr_n if not np.isnan(corr_n) else 0\n",
    "        features['template_g_corr'] = corr_g if not np.isnan(corr_g) else 0\n",
    "        \n",
    "        # L2 distance\n",
    "        features['template_n_l2'] = np.linalg.norm(pulse_trunc - template_n)\n",
    "        features['template_g_l2'] = np.linalg.norm(pulse_trunc - template_g)\n",
    "        \n",
    "        # Discrimination score\n",
    "        features['template_score'] = (corr_n - corr_g) + \\\n",
    "                                     (features['template_g_l2'] - features['template_n_l2'])\n",
    "        \n",
    "        # Gatti filter\n",
    "        if self.gatti_weights is not None:\n",
    "            features['gatti_score'] = np.dot(self.gatti_weights[:min_len], pulse_trunc)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _wavelet_features(self, pulse):\n",
    "        \"\"\"Wavelet decomposition features\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        try:\n",
    "            # Discrete wavelet transform\n",
    "            coeffs = pywt.wavedec(pulse, 'db4', level=4)\n",
    "            \n",
    "            # Energy in each level\n",
    "            for i, c in enumerate(coeffs):\n",
    "                features[f'wavelet_energy_L{i}'] = np.sum(c**2)\n",
    "            \n",
    "            # Wavelet entropy\n",
    "            energies = [np.sum(c**2) for c in coeffs]\n",
    "            total_energy = sum(energies)\n",
    "            if total_energy > 0:\n",
    "                probs = [e / total_energy for e in energies]\n",
    "                entropy = -sum([p * np.log2(p + 1e-10) for p in probs if p > 0])\n",
    "                features['wavelet_entropy'] = entropy\n",
    "            else:\n",
    "                features['wavelet_entropy'] = 0\n",
    "        except:\n",
    "            for i in range(5):\n",
    "                features[f'wavelet_energy_L{i}'] = 0\n",
    "            features['wavelet_entropy'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def set_templates(self, neutron_waveforms, gamma_waveforms):\n",
    "        \"\"\"Build templates from training data\"\"\"\n",
    "        self.neutron_template = self._build_template(neutron_waveforms)\n",
    "        self.gamma_template = self._build_template(gamma_waveforms)\n",
    "        \n",
    "        # Gatti optimal filter\n",
    "        diff = self.neutron_template - self.gamma_template\n",
    "        noise_var = self._estimate_noise_variance(neutron_waveforms, gamma_waveforms)\n",
    "        self.gatti_weights = diff / (noise_var + 1e-10)\n",
    "        \n",
    "        print(\"✓ Templates and Gatti filter computed\")\n",
    "    \n",
    "    def _build_template(self, waveforms):\n",
    "        \"\"\"Build average template from waveforms\"\"\"\n",
    "        templates = []\n",
    "        for wf in waveforms[:500]:  # Use subset\n",
    "            baseline = np.mean(wf[:self.baseline_samples])\n",
    "            pulse = baseline - wf\n",
    "            if np.max(pulse) > 100:\n",
    "                pulse_norm = pulse / np.max(pulse)\n",
    "                templates.append(pulse_norm)\n",
    "        \n",
    "        if templates:\n",
    "            return np.median(templates, axis=0)\n",
    "        else:\n",
    "            return np.zeros(waveforms.shape[1])\n",
    "    \n",
    "    def _estimate_noise_variance(self, wf_n, wf_g):\n",
    "        \"\"\"Estimate noise from baseline\"\"\"\n",
    "        baselines_n = wf_n[:, :self.baseline_samples]\n",
    "        baselines_g = wf_g[:, :self.baseline_samples]\n",
    "        return np.mean([baselines_n.var(), baselines_g.var()])\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = FeatureExtractor()\n",
    "\n",
    "print(\"✓ FeatureExtractor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Features from Example Waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "features_gamma = extractor.extract_all_features(gamma_wf)\n",
    "features_neutron = extractor.extract_all_features(neutron_wf)\n",
    "\n",
    "print(\"Feature extraction results:\\n\")\n",
    "print(f\"Total features extracted: {len(features_gamma)}\")\n",
    "print(f\"\\nKey discriminating features:\")\n",
    "print(f\"{'Feature':<30} {'Gamma':<12} {'Neutron':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "key_features = [\n",
    "    'psd_traditional',\n",
    "    'Q_ratio_200_800ns',\n",
    "    'charge_t50pct',\n",
    "    'charge_t90pct',\n",
    "    'decay_A_ratio',\n",
    "    'tot_50pct',\n",
    "    'rise_10_90'\n",
    "]\n",
    "\n",
    "for feat in key_features:\n",
    "    if feat in features_gamma and feat in features_neutron:\n",
    "        print(f\"{feat:<30} {features_gamma[feat]:<12.4f} {features_neutron[feat]:<12.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Features extracted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Templates and Extract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training dataset\n",
    "print(\"Generating training dataset...\")\n",
    "\n",
    "n_train = 1000  # Per particle type\n",
    "\n",
    "gamma_waveforms = []\n",
    "neutron_waveforms = []\n",
    "\n",
    "for i in range(n_train):\n",
    "    energy = np.random.exponential(400) + 50\n",
    "    energy = min(energy, 2000)\n",
    "    \n",
    "    gamma_waveforms.append(generate_waveform('gamma', energy))\n",
    "    neutron_waveforms.append(generate_waveform('neutron', energy))\n",
    "\n",
    "gamma_waveforms = np.array(gamma_waveforms)\n",
    "neutron_waveforms = np.array(neutron_waveforms)\n",
    "\n",
    "# Build templates\n",
    "extractor.set_templates(neutron_waveforms, gamma_waveforms)\n",
    "\n",
    "# Extract features for all waveforms\n",
    "print(\"Extracting features for all waveforms...\")\n",
    "\n",
    "all_features = []\n",
    "\n",
    "for i, wf in enumerate(gamma_waveforms):\n",
    "    features = extractor.extract_all_features(wf)\n",
    "    features['particle'] = 'gamma'\n",
    "    all_features.append(features)\n",
    "    \n",
    "    if (i+1) % 200 == 0:\n",
    "        print(f\"  Processed {i+1}/{n_train} gamma events\")\n",
    "\n",
    "for i, wf in enumerate(neutron_waveforms):\n",
    "    features = extractor.extract_all_features(wf)\n",
    "    features['particle'] = 'neutron'\n",
    "    all_features.append(features)\n",
    "    \n",
    "    if (i+1) % 200 == 0:\n",
    "        print(f\"  Processed {i+1}/{n_train} neutron events\")\n",
    "\n",
    "# Create DataFrame\n",
    "df_features = pd.DataFrame(all_features)\n",
    "\n",
    "print(f\"\\n✓ Feature extraction complete\")\n",
    "print(f\"  Dataset shape: {df_features.shape}\")\n",
    "print(f\"  Total features: {len(df_features.columns) - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric features\n",
    "numeric_features = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove QC flags\n",
    "numeric_features = [f for f in numeric_features if not f.startswith('n_') and \n",
    "                   f not in ['saturated', 'pile_up_likely']]\n",
    "\n",
    "# Calculate correlation with particle type\n",
    "df_features['particle_label'] = (df_features['particle'] == 'neutron').astype(int)\n",
    "\n",
    "correlations = []\n",
    "for feat in numeric_features:\n",
    "    if df_features[feat].std() > 0:  # Exclude constant features\n",
    "        corr = df_features[['particle_label', feat]].corr().iloc[0, 1]\n",
    "        correlations.append({'feature': feat, 'correlation': abs(corr)})\n",
    "\n",
    "corr_df = pd.DataFrame(correlations).sort_values('correlation', ascending=False)\n",
    "\n",
    "print(\"Top 20 most discriminating features:\\n\")\n",
    "print(corr_df.head(20).to_string(index=False))\n",
    "\n",
    "# Plot top features\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "top_features = corr_df.head(20)\n",
    "ax.barh(range(len(top_features)), top_features['correlation'], \n",
    "        color='steelblue', edgecolor='black', linewidth=1.2)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.set_xlabel('Absolute Correlation with Particle Type', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 20 Discriminating Features', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Feature importance analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Key Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of top features\n",
    "top_4_features = corr_df.head(4)['feature'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feat in enumerate(top_4_features):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    gamma_vals = df_features[df_features['particle'] == 'gamma'][feat]\n",
    "    neutron_vals = df_features[df_features['particle'] == 'neutron'][feat]\n",
    "    \n",
    "    # Remove outliers for better visualization\n",
    "    vmin = np.percentile(df_features[feat], 1)\n",
    "    vmax = np.percentile(df_features[feat], 99)\n",
    "    \n",
    "    bins = np.linspace(vmin, vmax, 50)\n",
    "    \n",
    "    ax.hist(gamma_vals, bins=bins, alpha=0.6, label='Gamma', \n",
    "           color='blue', edgecolor='black', linewidth=0.5)\n",
    "    ax.hist(neutron_vals, bins=bins, alpha=0.6, label='Neutron', \n",
    "           color='red', edgecolor='black', linewidth=0.5)\n",
    "    \n",
    "    ax.set_xlabel(feat, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Counts', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{feat}\\n(corr = {corr_df[corr_df[\"feature\"]==feat][\"correlation\"].values[0]:.3f})',\n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Feature distributions plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Comprehensive Feature Extraction**: Extracted 100+ timing features from each waveform\n",
    "   - Multiple charge ratios capture energy-dependent behavior\n",
    "   - Cumulative charge timestamps reveal charge collection dynamics\n",
    "   - Bi-exponential fits measure fast/slow component ratio\n",
    "   - Frequency domain features capture oscillatory behavior\n",
    "   - Wavelet features provide multi-resolution analysis\n",
    "\n",
    "2. **Template Matching**: Built particle-specific templates\n",
    "   - Correlation measures shape similarity\n",
    "   - Gatti filter is the optimal linear discriminator\n",
    "   - Physics-informed approach improves low-energy performance\n",
    "\n",
    "3. **Feature Importance**: Identified most discriminating features\n",
    "   - Charge ratio features consistently rank highest\n",
    "   - Tail-related features (cumulative charge times) are critical\n",
    "   - Redundancy provides robustness\n",
    "\n",
    "4. **Quality Control**: Detect problematic events\n",
    "   - Saturation detection prevents calibration errors\n",
    "   - Pile-up detection removes multi-event pulses\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Feature Selection**: Use correlation analysis to identify top features\n",
    "- **Normalization**: Essential before machine learning (next notebook)\n",
    "- **Template Building**: Use large, clean dataset (1000+ events)\n",
    "- **Energy Dependence**: Features may perform differently at different energies\n",
    "- **Computational Cost**: For real-time applications, select subset of features\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In Notebook 4, we'll use these features to train machine learning classifiers for improved neutron/gamma discrimination."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
